{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pangeo Storage Benchmarks\n",
    "\n",
    "This is more in depth documentation and discussion of the Pangeo Storage Benchmark tests and some information that will serve as discussion around development of tests and also precursor material for future papers or publications that may result. \n",
    "\n",
    "Benchmarks are meant to systematically determine performance characteristics of storage backend/API combinations for processing geoscience data on clustered cloud computing resources based on Xarray/Dask within the Python scientific computing framework. Focus will be primarily on read performance although write tests will be written as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Cases\n",
    "Here is an overview of the use cases covered or in development. \n",
    "\n",
    "|API/User Interface  | Environment       | format       | Storage Layer     |  \n",
    "|:------------------ |:----------------- |:-------------|:------------------|\n",
    "| Dask/Xarray        | GCP               | Zarr         | GCS via GCSFS     |\n",
    "|                    |                   |              | FUSE              |\n",
    "|                    |                   | NetCDF4      | FUSE              |\n",
    "|                    |                   | H5NetCDF     | HSDS/GCS          |\n",
    "|                    | AWS               | Zarr         | S3 via            |\n",
    "|                    |                   | NetCDF4      |                   |\n",
    "|                    |                   | H5NetCDF     | HSDS/S3           |\n",
    "|                    | Azure             | TBD          |                   |\n",
    "|                    | GCP               | TileDB       | TBD               |\n",
    "| Numpy              | GCP               | Zarr         | GCS via GCSFS     |\n",
    "|                    |                   |              | FUSE              |\n",
    "|                    |                   | NetCDF4      | FUSE              |\n",
    "|                    |                   | H5NetCDF     | HSDS/GCS          |\n",
    "|                    | AWS               | Zarr         | S3 via            |\n",
    "|                    |                   | NetCDF4      |                   |\n",
    "|                    |                   | H5NetCDF     | HSDS/S3           |\n",
    "|                    | Azure             |              |                   | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarks\n",
    "---\n",
    "[ASV](https://asv.readthedocs.io/en/latest/index.html) the framework used to run the\n",
    "benchmarks and collect results. For this repo, results are kept in `benchmarks` the \n",
    "directory in JSON format. The benchmarks are roughly separated into two categories: distributed tests which are run clustered via Dask across multiple processors and/or machines\n",
    "\n",
    "## Environments\n",
    "---\n",
    "#### Cloud Platforms (GCP/AWS/Azure)\n",
    "These configurations comprise Kubernetes/Container based Dask clusters where a configurable number of workers have read/write access to massively parallelized object storage. \n",
    "\n",
    "#### Server/HPC\n",
    "Typical server or HPC environment found at research institutions. \n",
    "\n",
    "#### Single Workstation\n",
    "A subset of tests run on laptop/workstation class hardware.\n",
    "\n",
    "## Datasets\n",
    "---\n",
    "#### Dask/Xarray Synthetic\n",
    "A randomly generated three dimensional dataset with shape (1350, 1000, 1000) \n",
    "is produced for each run which generates roughly 10 GB of data. Tests are run across \n",
    "a variety of parameters including chunk configuration, number of dask workers, and... \n",
    "\n",
    "```\n",
    "chunks = (10, 1000, 1000)\n",
    "size = (1350, 1000, 1000)\n",
    "dask_arr = da.random.normal(10, 0.1, size=size, chunks=chunks)\n",
    "```\n",
    "Chunks parameter varies configuration across the x-axis. The following executions are individually timed. In the current configuration, each test is averaged over five runs with three runs per test.\n",
    "\n",
    "```\n",
    "# Mean compute test\n",
    "dask_arr.mean().compute()\n",
    "# Write test\n",
    "dask_arr.store(zarr_ds, lock=False)\n",
    "```\n",
    "\n",
    "#### Real World Tests\n",
    "\n",
    "##### LOCA\n",
    "\n",
    "\n",
    "##### LLC4320\n",
    "MITgcm global ocean model output from a $1/48^{\\circ}$ model (LLC4320) is used as real world data input. Data resides in `storage-benchmarks` buckets on Pangeo environments in various formats. Currently have data as NetCDF files accessible via FUSE or on GCS object store in Zarr format with a pre-determined chunk configuration chosen to be roughly 10 MB in size per chunk.\n",
    "\n",
    "### Local Tests\n",
    "\n",
    "\n",
    "Randomly generated 3D Numpy array which defaults to $n=32$ across $z$ axis. This generates \n",
    "a 250 MB dataset to run the synthetic tests.\n",
    "\n",
    "```\n",
    "def readtest(f):\n",
    "    \"\"\"Read all values of dataset and confirm they are in the expected range\n",
    "    \"\"\"\n",
    "    dset = f[_DATASET_NAME]\n",
    "    nz = dset.shape[0]\n",
    "    for i in range(nz):\n",
    "        arr = dset[i,:,:]\n",
    "        mean = arr.mean()\n",
    "        if mean < 0.4 or mean > 0.6:\n",
    "            msg = \"mean of {} for slice: {} is unexpected\".format(mean, i)\n",
    "            raise ValueError(msg)\n",
    "\n",
    "\n",
    "def writetest(f, data):\n",
    "    \"\"\"Update all values of the dataset\n",
    "    \"\"\"\n",
    "    dset = f[_DATASET_NAME]\n",
    "    nz = dset.shape[0]\n",
    "    for i in range(nz):\n",
    "        dset[i,:,:] = data[i,:,:]\n",
    "\n",
    "def tasmax_slicetest(f):\n",
    "    \"\"\" Check random slice of tasmax dataset\n",
    "    \n",
    "    \"\"\"\n",
    "    dset = f['tasmax']\n",
    "    day = random.randrange(dset.shape[0])  # choose random day in year\n",
    "    data = dset[day,:,:]  # get numpy array for given day\n",
    "    vals = data[np.where(data<400.0)]  # cull fill values\n",
    "    min = vals.min()\n",
    "    if min < 100.0:\n",
    "        msg = \"day: {} was unusually cold! (for degrees kelvin)\".format(day)\n",
    "        raise ValueError(msg)\n",
    "    max = vals.max()\n",
    "    if max > 350.0:\n",
    "        msg = \"day: {} was unusually hot! (for degrees kelvin)\".format(day)\n",
    "        raise ValueError(msg)\n",
    "    if max - min < 20.0:\n",
    "        msg = \"day: {} expected more variation\".format(day)\n",
    "raise ValueError(msg)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
